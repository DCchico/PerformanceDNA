# FX Graph for bert-base-uncased
# (Generated by torch._dynamo.export)




def forward(self, input_ids):
    arg0, = fx_pytree.tree_flatten_spec(([input_ids], {}), self._in_spec)
    l_input_ids_ = arg0
    size = l_input_ids_.size()
    getitem_1 = size[1];  size = None
    l__self___embeddings_token_type_ids = self.L__self___embeddings_token_type_ids
    buffered_token_type_ids = l__self___embeddings_token_type_ids[(slice(None, None, None), slice(None, getitem_1, None))];  l__self___embeddings_token_type_ids = None
    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(1, getitem_1);  buffered_token_type_ids = None
    l__self___embeddings_position_ids = self.L__self___embeddings_position_ids
    position_ids = l__self___embeddings_position_ids[(slice(None, None, None), slice(0, 512, None))];  l__self___embeddings_position_ids = None
    inputs_embeds = self.L__self___embeddings_word_embeddings(l_input_ids_);  l_input_ids_ = None
    token_type_embeddings = self.L__self___embeddings_token_type_embeddings(buffered_token_type_ids_expanded);  buffered_token_type_ids_expanded = None
    embeddings = inputs_embeds + token_type_embeddings;  inputs_embeds = token_type_embeddings = None
    position_embeddings = self.L__self___embeddings_position_embeddings(position_ids);  position_ids = None
    embeddings += position_embeddings;  embeddings_1 = embeddings;  embeddings = position_embeddings = None
    embeddings_2 = self.L__self___embeddings_LayerNorm(embeddings_1);  embeddings_1 = None
    embeddings_3 = self.L__self___embeddings_dropout(embeddings_2);  embeddings_2 = None
    add_1 = getitem_1 + 0
    attention_mask = torch.ones((1, add_1), device = device(type='cuda', index=0));  add_1 = None
    getitem_4 = attention_mask[(slice(None, None, None), None, None, slice(None, None, None))];  attention_mask = None
    expand_1 = getitem_4.expand(1, 1, getitem_1, 512);  getitem_4 = getitem_1 = None
    expanded_mask = expand_1.to(torch.float32);  expand_1 = None
    inverted_mask = 1.0 - expanded_mask;  expanded_mask = None
    to_1 = inverted_mask.to(torch.bool)
    extended_attention_mask = inverted_mask.masked_fill(to_1, -3.4028234663852886e+38);  inverted_mask = to_1 = None
    l__self___encoder_layer_0_attention_self_query = self.L__self___encoder_layer_0_attention_self_query(embeddings_3)
    x = l__self___encoder_layer_0_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_0_attention_self_query = None
    query_layer = x.permute(0, 2, 1, 3);  x = None
    l__self___encoder_layer_0_attention_self_key = self.L__self___encoder_layer_0_attention_self_key(embeddings_3)
    x_1 = l__self___encoder_layer_0_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_0_attention_self_key = None
    key_layer = x_1.permute(0, 2, 1, 3);  x_1 = None
    l__self___encoder_layer_0_attention_self_value = self.L__self___encoder_layer_0_attention_self_value(embeddings_3)
    x_2 = l__self___encoder_layer_0_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_0_attention_self_value = None
    value_layer = x_2.permute(0, 2, 1, 3);  x_2 = None
    attn_output = torch._C._nn.scaled_dot_product_attention(query_layer, key_layer, value_layer, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer = key_layer = value_layer = None
    attn_output_1 = attn_output.transpose(1, 2);  attn_output = None
    attn_output_2 = attn_output_1.reshape(1, 512, 768);  attn_output_1 = None
    hidden_states = self.L__self___encoder_layer_0_attention_output_dense(attn_output_2);  attn_output_2 = None
    hidden_states_1 = self.L__self___encoder_layer_0_attention_output_dropout(hidden_states);  hidden_states = None
    add_2 = hidden_states_1 + embeddings_3;  hidden_states_1 = embeddings_3 = None
    hidden_states_2 = self.L__self___encoder_layer_0_attention_output_LayerNorm(add_2);  add_2 = None
    hidden_states_3 = self.L__self___encoder_layer_0_intermediate_dense(hidden_states_2)
    hidden_states_4 = torch._C._nn.gelu(hidden_states_3);  hidden_states_3 = None
    hidden_states_5 = self.L__self___encoder_layer_0_output_dense(hidden_states_4);  hidden_states_4 = None
    hidden_states_6 = self.L__self___encoder_layer_0_output_dropout(hidden_states_5);  hidden_states_5 = None
    add_3 = hidden_states_6 + hidden_states_2;  hidden_states_6 = hidden_states_2 = None
    hidden_states_7 = self.L__self___encoder_layer_0_output_LayerNorm(add_3);  add_3 = None
    l__self___encoder_layer_1_attention_self_query = self.L__self___encoder_layer_1_attention_self_query(hidden_states_7)
    x_3 = l__self___encoder_layer_1_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_1_attention_self_query = None
    query_layer_1 = x_3.permute(0, 2, 1, 3);  x_3 = None
    l__self___encoder_layer_1_attention_self_key = self.L__self___encoder_layer_1_attention_self_key(hidden_states_7)
    x_4 = l__self___encoder_layer_1_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_1_attention_self_key = None
    key_layer_1 = x_4.permute(0, 2, 1, 3);  x_4 = None
    l__self___encoder_layer_1_attention_self_value = self.L__self___encoder_layer_1_attention_self_value(hidden_states_7)
    x_5 = l__self___encoder_layer_1_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_1_attention_self_value = None
    value_layer_1 = x_5.permute(0, 2, 1, 3);  x_5 = None
    attn_output_3 = torch._C._nn.scaled_dot_product_attention(query_layer_1, key_layer_1, value_layer_1, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_1 = key_layer_1 = value_layer_1 = None
    attn_output_4 = attn_output_3.transpose(1, 2);  attn_output_3 = None
    attn_output_5 = attn_output_4.reshape(1, 512, 768);  attn_output_4 = None
    hidden_states_8 = self.L__self___encoder_layer_1_attention_output_dense(attn_output_5);  attn_output_5 = None
    hidden_states_9 = self.L__self___encoder_layer_1_attention_output_dropout(hidden_states_8);  hidden_states_8 = None
    add_4 = hidden_states_9 + hidden_states_7;  hidden_states_9 = hidden_states_7 = None
    hidden_states_10 = self.L__self___encoder_layer_1_attention_output_LayerNorm(add_4);  add_4 = None
    hidden_states_11 = self.L__self___encoder_layer_1_intermediate_dense(hidden_states_10)
    hidden_states_12 = torch._C._nn.gelu(hidden_states_11);  hidden_states_11 = None
    hidden_states_13 = self.L__self___encoder_layer_1_output_dense(hidden_states_12);  hidden_states_12 = None
    hidden_states_14 = self.L__self___encoder_layer_1_output_dropout(hidden_states_13);  hidden_states_13 = None
    add_5 = hidden_states_14 + hidden_states_10;  hidden_states_14 = hidden_states_10 = None
    hidden_states_15 = self.L__self___encoder_layer_1_output_LayerNorm(add_5);  add_5 = None
    l__self___encoder_layer_2_attention_self_query = self.L__self___encoder_layer_2_attention_self_query(hidden_states_15)
    x_6 = l__self___encoder_layer_2_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_2_attention_self_query = None
    query_layer_2 = x_6.permute(0, 2, 1, 3);  x_6 = None
    l__self___encoder_layer_2_attention_self_key = self.L__self___encoder_layer_2_attention_self_key(hidden_states_15)
    x_7 = l__self___encoder_layer_2_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_2_attention_self_key = None
    key_layer_2 = x_7.permute(0, 2, 1, 3);  x_7 = None
    l__self___encoder_layer_2_attention_self_value = self.L__self___encoder_layer_2_attention_self_value(hidden_states_15)
    x_8 = l__self___encoder_layer_2_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_2_attention_self_value = None
    value_layer_2 = x_8.permute(0, 2, 1, 3);  x_8 = None
    attn_output_6 = torch._C._nn.scaled_dot_product_attention(query_layer_2, key_layer_2, value_layer_2, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_2 = key_layer_2 = value_layer_2 = None
    attn_output_7 = attn_output_6.transpose(1, 2);  attn_output_6 = None
    attn_output_8 = attn_output_7.reshape(1, 512, 768);  attn_output_7 = None
    hidden_states_16 = self.L__self___encoder_layer_2_attention_output_dense(attn_output_8);  attn_output_8 = None
    hidden_states_17 = self.L__self___encoder_layer_2_attention_output_dropout(hidden_states_16);  hidden_states_16 = None
    add_6 = hidden_states_17 + hidden_states_15;  hidden_states_17 = hidden_states_15 = None
    hidden_states_18 = self.L__self___encoder_layer_2_attention_output_LayerNorm(add_6);  add_6 = None
    hidden_states_19 = self.L__self___encoder_layer_2_intermediate_dense(hidden_states_18)
    hidden_states_20 = torch._C._nn.gelu(hidden_states_19);  hidden_states_19 = None
    hidden_states_21 = self.L__self___encoder_layer_2_output_dense(hidden_states_20);  hidden_states_20 = None
    hidden_states_22 = self.L__self___encoder_layer_2_output_dropout(hidden_states_21);  hidden_states_21 = None
    add_7 = hidden_states_22 + hidden_states_18;  hidden_states_22 = hidden_states_18 = None
    hidden_states_23 = self.L__self___encoder_layer_2_output_LayerNorm(add_7);  add_7 = None
    l__self___encoder_layer_3_attention_self_query = self.L__self___encoder_layer_3_attention_self_query(hidden_states_23)
    x_9 = l__self___encoder_layer_3_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_3_attention_self_query = None
    query_layer_3 = x_9.permute(0, 2, 1, 3);  x_9 = None
    l__self___encoder_layer_3_attention_self_key = self.L__self___encoder_layer_3_attention_self_key(hidden_states_23)
    x_10 = l__self___encoder_layer_3_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_3_attention_self_key = None
    key_layer_3 = x_10.permute(0, 2, 1, 3);  x_10 = None
    l__self___encoder_layer_3_attention_self_value = self.L__self___encoder_layer_3_attention_self_value(hidden_states_23)
    x_11 = l__self___encoder_layer_3_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_3_attention_self_value = None
    value_layer_3 = x_11.permute(0, 2, 1, 3);  x_11 = None
    attn_output_9 = torch._C._nn.scaled_dot_product_attention(query_layer_3, key_layer_3, value_layer_3, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_3 = key_layer_3 = value_layer_3 = None
    attn_output_10 = attn_output_9.transpose(1, 2);  attn_output_9 = None
    attn_output_11 = attn_output_10.reshape(1, 512, 768);  attn_output_10 = None
    hidden_states_24 = self.L__self___encoder_layer_3_attention_output_dense(attn_output_11);  attn_output_11 = None
    hidden_states_25 = self.L__self___encoder_layer_3_attention_output_dropout(hidden_states_24);  hidden_states_24 = None
    add_8 = hidden_states_25 + hidden_states_23;  hidden_states_25 = hidden_states_23 = None
    hidden_states_26 = self.L__self___encoder_layer_3_attention_output_LayerNorm(add_8);  add_8 = None
    hidden_states_27 = self.L__self___encoder_layer_3_intermediate_dense(hidden_states_26)
    hidden_states_28 = torch._C._nn.gelu(hidden_states_27);  hidden_states_27 = None
    hidden_states_29 = self.L__self___encoder_layer_3_output_dense(hidden_states_28);  hidden_states_28 = None
    hidden_states_30 = self.L__self___encoder_layer_3_output_dropout(hidden_states_29);  hidden_states_29 = None
    add_9 = hidden_states_30 + hidden_states_26;  hidden_states_30 = hidden_states_26 = None
    hidden_states_31 = self.L__self___encoder_layer_3_output_LayerNorm(add_9);  add_9 = None
    l__self___encoder_layer_4_attention_self_query = self.L__self___encoder_layer_4_attention_self_query(hidden_states_31)
    x_12 = l__self___encoder_layer_4_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_4_attention_self_query = None
    query_layer_4 = x_12.permute(0, 2, 1, 3);  x_12 = None
    l__self___encoder_layer_4_attention_self_key = self.L__self___encoder_layer_4_attention_self_key(hidden_states_31)
    x_13 = l__self___encoder_layer_4_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_4_attention_self_key = None
    key_layer_4 = x_13.permute(0, 2, 1, 3);  x_13 = None
    l__self___encoder_layer_4_attention_self_value = self.L__self___encoder_layer_4_attention_self_value(hidden_states_31)
    x_14 = l__self___encoder_layer_4_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_4_attention_self_value = None
    value_layer_4 = x_14.permute(0, 2, 1, 3);  x_14 = None
    attn_output_12 = torch._C._nn.scaled_dot_product_attention(query_layer_4, key_layer_4, value_layer_4, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_4 = key_layer_4 = value_layer_4 = None
    attn_output_13 = attn_output_12.transpose(1, 2);  attn_output_12 = None
    attn_output_14 = attn_output_13.reshape(1, 512, 768);  attn_output_13 = None
    hidden_states_32 = self.L__self___encoder_layer_4_attention_output_dense(attn_output_14);  attn_output_14 = None
    hidden_states_33 = self.L__self___encoder_layer_4_attention_output_dropout(hidden_states_32);  hidden_states_32 = None
    add_10 = hidden_states_33 + hidden_states_31;  hidden_states_33 = hidden_states_31 = None
    hidden_states_34 = self.L__self___encoder_layer_4_attention_output_LayerNorm(add_10);  add_10 = None
    hidden_states_35 = self.L__self___encoder_layer_4_intermediate_dense(hidden_states_34)
    hidden_states_36 = torch._C._nn.gelu(hidden_states_35);  hidden_states_35 = None
    hidden_states_37 = self.L__self___encoder_layer_4_output_dense(hidden_states_36);  hidden_states_36 = None
    hidden_states_38 = self.L__self___encoder_layer_4_output_dropout(hidden_states_37);  hidden_states_37 = None
    add_11 = hidden_states_38 + hidden_states_34;  hidden_states_38 = hidden_states_34 = None
    hidden_states_39 = self.L__self___encoder_layer_4_output_LayerNorm(add_11);  add_11 = None
    l__self___encoder_layer_5_attention_self_query = self.L__self___encoder_layer_5_attention_self_query(hidden_states_39)
    x_15 = l__self___encoder_layer_5_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_5_attention_self_query = None
    query_layer_5 = x_15.permute(0, 2, 1, 3);  x_15 = None
    l__self___encoder_layer_5_attention_self_key = self.L__self___encoder_layer_5_attention_self_key(hidden_states_39)
    x_16 = l__self___encoder_layer_5_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_5_attention_self_key = None
    key_layer_5 = x_16.permute(0, 2, 1, 3);  x_16 = None
    l__self___encoder_layer_5_attention_self_value = self.L__self___encoder_layer_5_attention_self_value(hidden_states_39)
    x_17 = l__self___encoder_layer_5_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_5_attention_self_value = None
    value_layer_5 = x_17.permute(0, 2, 1, 3);  x_17 = None
    attn_output_15 = torch._C._nn.scaled_dot_product_attention(query_layer_5, key_layer_5, value_layer_5, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_5 = key_layer_5 = value_layer_5 = None
    attn_output_16 = attn_output_15.transpose(1, 2);  attn_output_15 = None
    attn_output_17 = attn_output_16.reshape(1, 512, 768);  attn_output_16 = None
    hidden_states_40 = self.L__self___encoder_layer_5_attention_output_dense(attn_output_17);  attn_output_17 = None
    hidden_states_41 = self.L__self___encoder_layer_5_attention_output_dropout(hidden_states_40);  hidden_states_40 = None
    add_12 = hidden_states_41 + hidden_states_39;  hidden_states_41 = hidden_states_39 = None
    hidden_states_42 = self.L__self___encoder_layer_5_attention_output_LayerNorm(add_12);  add_12 = None
    hidden_states_43 = self.L__self___encoder_layer_5_intermediate_dense(hidden_states_42)
    hidden_states_44 = torch._C._nn.gelu(hidden_states_43);  hidden_states_43 = None
    hidden_states_45 = self.L__self___encoder_layer_5_output_dense(hidden_states_44);  hidden_states_44 = None
    hidden_states_46 = self.L__self___encoder_layer_5_output_dropout(hidden_states_45);  hidden_states_45 = None
    add_13 = hidden_states_46 + hidden_states_42;  hidden_states_46 = hidden_states_42 = None
    hidden_states_47 = self.L__self___encoder_layer_5_output_LayerNorm(add_13);  add_13 = None
    l__self___encoder_layer_6_attention_self_query = self.L__self___encoder_layer_6_attention_self_query(hidden_states_47)
    x_18 = l__self___encoder_layer_6_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_6_attention_self_query = None
    query_layer_6 = x_18.permute(0, 2, 1, 3);  x_18 = None
    l__self___encoder_layer_6_attention_self_key = self.L__self___encoder_layer_6_attention_self_key(hidden_states_47)
    x_19 = l__self___encoder_layer_6_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_6_attention_self_key = None
    key_layer_6 = x_19.permute(0, 2, 1, 3);  x_19 = None
    l__self___encoder_layer_6_attention_self_value = self.L__self___encoder_layer_6_attention_self_value(hidden_states_47)
    x_20 = l__self___encoder_layer_6_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_6_attention_self_value = None
    value_layer_6 = x_20.permute(0, 2, 1, 3);  x_20 = None
    attn_output_18 = torch._C._nn.scaled_dot_product_attention(query_layer_6, key_layer_6, value_layer_6, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_6 = key_layer_6 = value_layer_6 = None
    attn_output_19 = attn_output_18.transpose(1, 2);  attn_output_18 = None
    attn_output_20 = attn_output_19.reshape(1, 512, 768);  attn_output_19 = None
    hidden_states_48 = self.L__self___encoder_layer_6_attention_output_dense(attn_output_20);  attn_output_20 = None
    hidden_states_49 = self.L__self___encoder_layer_6_attention_output_dropout(hidden_states_48);  hidden_states_48 = None
    add_14 = hidden_states_49 + hidden_states_47;  hidden_states_49 = hidden_states_47 = None
    hidden_states_50 = self.L__self___encoder_layer_6_attention_output_LayerNorm(add_14);  add_14 = None
    hidden_states_51 = self.L__self___encoder_layer_6_intermediate_dense(hidden_states_50)
    hidden_states_52 = torch._C._nn.gelu(hidden_states_51);  hidden_states_51 = None
    hidden_states_53 = self.L__self___encoder_layer_6_output_dense(hidden_states_52);  hidden_states_52 = None
    hidden_states_54 = self.L__self___encoder_layer_6_output_dropout(hidden_states_53);  hidden_states_53 = None
    add_15 = hidden_states_54 + hidden_states_50;  hidden_states_54 = hidden_states_50 = None
    hidden_states_55 = self.L__self___encoder_layer_6_output_LayerNorm(add_15);  add_15 = None
    l__self___encoder_layer_7_attention_self_query = self.L__self___encoder_layer_7_attention_self_query(hidden_states_55)
    x_21 = l__self___encoder_layer_7_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_7_attention_self_query = None
    query_layer_7 = x_21.permute(0, 2, 1, 3);  x_21 = None
    l__self___encoder_layer_7_attention_self_key = self.L__self___encoder_layer_7_attention_self_key(hidden_states_55)
    x_22 = l__self___encoder_layer_7_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_7_attention_self_key = None
    key_layer_7 = x_22.permute(0, 2, 1, 3);  x_22 = None
    l__self___encoder_layer_7_attention_self_value = self.L__self___encoder_layer_7_attention_self_value(hidden_states_55)
    x_23 = l__self___encoder_layer_7_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_7_attention_self_value = None
    value_layer_7 = x_23.permute(0, 2, 1, 3);  x_23 = None
    attn_output_21 = torch._C._nn.scaled_dot_product_attention(query_layer_7, key_layer_7, value_layer_7, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_7 = key_layer_7 = value_layer_7 = None
    attn_output_22 = attn_output_21.transpose(1, 2);  attn_output_21 = None
    attn_output_23 = attn_output_22.reshape(1, 512, 768);  attn_output_22 = None
    hidden_states_56 = self.L__self___encoder_layer_7_attention_output_dense(attn_output_23);  attn_output_23 = None
    hidden_states_57 = self.L__self___encoder_layer_7_attention_output_dropout(hidden_states_56);  hidden_states_56 = None
    add_16 = hidden_states_57 + hidden_states_55;  hidden_states_57 = hidden_states_55 = None
    hidden_states_58 = self.L__self___encoder_layer_7_attention_output_LayerNorm(add_16);  add_16 = None
    hidden_states_59 = self.L__self___encoder_layer_7_intermediate_dense(hidden_states_58)
    hidden_states_60 = torch._C._nn.gelu(hidden_states_59);  hidden_states_59 = None
    hidden_states_61 = self.L__self___encoder_layer_7_output_dense(hidden_states_60);  hidden_states_60 = None
    hidden_states_62 = self.L__self___encoder_layer_7_output_dropout(hidden_states_61);  hidden_states_61 = None
    add_17 = hidden_states_62 + hidden_states_58;  hidden_states_62 = hidden_states_58 = None
    hidden_states_63 = self.L__self___encoder_layer_7_output_LayerNorm(add_17);  add_17 = None
    l__self___encoder_layer_8_attention_self_query = self.L__self___encoder_layer_8_attention_self_query(hidden_states_63)
    x_24 = l__self___encoder_layer_8_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_8_attention_self_query = None
    query_layer_8 = x_24.permute(0, 2, 1, 3);  x_24 = None
    l__self___encoder_layer_8_attention_self_key = self.L__self___encoder_layer_8_attention_self_key(hidden_states_63)
    x_25 = l__self___encoder_layer_8_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_8_attention_self_key = None
    key_layer_8 = x_25.permute(0, 2, 1, 3);  x_25 = None
    l__self___encoder_layer_8_attention_self_value = self.L__self___encoder_layer_8_attention_self_value(hidden_states_63)
    x_26 = l__self___encoder_layer_8_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_8_attention_self_value = None
    value_layer_8 = x_26.permute(0, 2, 1, 3);  x_26 = None
    attn_output_24 = torch._C._nn.scaled_dot_product_attention(query_layer_8, key_layer_8, value_layer_8, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_8 = key_layer_8 = value_layer_8 = None
    attn_output_25 = attn_output_24.transpose(1, 2);  attn_output_24 = None
    attn_output_26 = attn_output_25.reshape(1, 512, 768);  attn_output_25 = None
    hidden_states_64 = self.L__self___encoder_layer_8_attention_output_dense(attn_output_26);  attn_output_26 = None
    hidden_states_65 = self.L__self___encoder_layer_8_attention_output_dropout(hidden_states_64);  hidden_states_64 = None
    add_18 = hidden_states_65 + hidden_states_63;  hidden_states_65 = hidden_states_63 = None
    hidden_states_66 = self.L__self___encoder_layer_8_attention_output_LayerNorm(add_18);  add_18 = None
    hidden_states_67 = self.L__self___encoder_layer_8_intermediate_dense(hidden_states_66)
    hidden_states_68 = torch._C._nn.gelu(hidden_states_67);  hidden_states_67 = None
    hidden_states_69 = self.L__self___encoder_layer_8_output_dense(hidden_states_68);  hidden_states_68 = None
    hidden_states_70 = self.L__self___encoder_layer_8_output_dropout(hidden_states_69);  hidden_states_69 = None
    add_19 = hidden_states_70 + hidden_states_66;  hidden_states_70 = hidden_states_66 = None
    hidden_states_71 = self.L__self___encoder_layer_8_output_LayerNorm(add_19);  add_19 = None
    l__self___encoder_layer_9_attention_self_query = self.L__self___encoder_layer_9_attention_self_query(hidden_states_71)
    x_27 = l__self___encoder_layer_9_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_9_attention_self_query = None
    query_layer_9 = x_27.permute(0, 2, 1, 3);  x_27 = None
    l__self___encoder_layer_9_attention_self_key = self.L__self___encoder_layer_9_attention_self_key(hidden_states_71)
    x_28 = l__self___encoder_layer_9_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_9_attention_self_key = None
    key_layer_9 = x_28.permute(0, 2, 1, 3);  x_28 = None
    l__self___encoder_layer_9_attention_self_value = self.L__self___encoder_layer_9_attention_self_value(hidden_states_71)
    x_29 = l__self___encoder_layer_9_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_9_attention_self_value = None
    value_layer_9 = x_29.permute(0, 2, 1, 3);  x_29 = None
    attn_output_27 = torch._C._nn.scaled_dot_product_attention(query_layer_9, key_layer_9, value_layer_9, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_9 = key_layer_9 = value_layer_9 = None
    attn_output_28 = attn_output_27.transpose(1, 2);  attn_output_27 = None
    attn_output_29 = attn_output_28.reshape(1, 512, 768);  attn_output_28 = None
    hidden_states_72 = self.L__self___encoder_layer_9_attention_output_dense(attn_output_29);  attn_output_29 = None
    hidden_states_73 = self.L__self___encoder_layer_9_attention_output_dropout(hidden_states_72);  hidden_states_72 = None
    add_20 = hidden_states_73 + hidden_states_71;  hidden_states_73 = hidden_states_71 = None
    hidden_states_74 = self.L__self___encoder_layer_9_attention_output_LayerNorm(add_20);  add_20 = None
    hidden_states_75 = self.L__self___encoder_layer_9_intermediate_dense(hidden_states_74)
    hidden_states_76 = torch._C._nn.gelu(hidden_states_75);  hidden_states_75 = None
    hidden_states_77 = self.L__self___encoder_layer_9_output_dense(hidden_states_76);  hidden_states_76 = None
    hidden_states_78 = self.L__self___encoder_layer_9_output_dropout(hidden_states_77);  hidden_states_77 = None
    add_21 = hidden_states_78 + hidden_states_74;  hidden_states_78 = hidden_states_74 = None
    hidden_states_79 = self.L__self___encoder_layer_9_output_LayerNorm(add_21);  add_21 = None
    l__self___encoder_layer_10_attention_self_query = self.L__self___encoder_layer_10_attention_self_query(hidden_states_79)
    x_30 = l__self___encoder_layer_10_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_10_attention_self_query = None
    query_layer_10 = x_30.permute(0, 2, 1, 3);  x_30 = None
    l__self___encoder_layer_10_attention_self_key = self.L__self___encoder_layer_10_attention_self_key(hidden_states_79)
    x_31 = l__self___encoder_layer_10_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_10_attention_self_key = None
    key_layer_10 = x_31.permute(0, 2, 1, 3);  x_31 = None
    l__self___encoder_layer_10_attention_self_value = self.L__self___encoder_layer_10_attention_self_value(hidden_states_79)
    x_32 = l__self___encoder_layer_10_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_10_attention_self_value = None
    value_layer_10 = x_32.permute(0, 2, 1, 3);  x_32 = None
    attn_output_30 = torch._C._nn.scaled_dot_product_attention(query_layer_10, key_layer_10, value_layer_10, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_10 = key_layer_10 = value_layer_10 = None
    attn_output_31 = attn_output_30.transpose(1, 2);  attn_output_30 = None
    attn_output_32 = attn_output_31.reshape(1, 512, 768);  attn_output_31 = None
    hidden_states_80 = self.L__self___encoder_layer_10_attention_output_dense(attn_output_32);  attn_output_32 = None
    hidden_states_81 = self.L__self___encoder_layer_10_attention_output_dropout(hidden_states_80);  hidden_states_80 = None
    add_22 = hidden_states_81 + hidden_states_79;  hidden_states_81 = hidden_states_79 = None
    hidden_states_82 = self.L__self___encoder_layer_10_attention_output_LayerNorm(add_22);  add_22 = None
    hidden_states_83 = self.L__self___encoder_layer_10_intermediate_dense(hidden_states_82)
    hidden_states_84 = torch._C._nn.gelu(hidden_states_83);  hidden_states_83 = None
    hidden_states_85 = self.L__self___encoder_layer_10_output_dense(hidden_states_84);  hidden_states_84 = None
    hidden_states_86 = self.L__self___encoder_layer_10_output_dropout(hidden_states_85);  hidden_states_85 = None
    add_23 = hidden_states_86 + hidden_states_82;  hidden_states_86 = hidden_states_82 = None
    hidden_states_87 = self.L__self___encoder_layer_10_output_LayerNorm(add_23);  add_23 = None
    l__self___encoder_layer_11_attention_self_query = self.L__self___encoder_layer_11_attention_self_query(hidden_states_87)
    x_33 = l__self___encoder_layer_11_attention_self_query.view((1, 512, 12, 64));  l__self___encoder_layer_11_attention_self_query = None
    query_layer_11 = x_33.permute(0, 2, 1, 3);  x_33 = None
    l__self___encoder_layer_11_attention_self_key = self.L__self___encoder_layer_11_attention_self_key(hidden_states_87)
    x_34 = l__self___encoder_layer_11_attention_self_key.view((1, 512, 12, 64));  l__self___encoder_layer_11_attention_self_key = None
    key_layer_11 = x_34.permute(0, 2, 1, 3);  x_34 = None
    l__self___encoder_layer_11_attention_self_value = self.L__self___encoder_layer_11_attention_self_value(hidden_states_87)
    x_35 = l__self___encoder_layer_11_attention_self_value.view((1, 512, 12, 64));  l__self___encoder_layer_11_attention_self_value = None
    value_layer_11 = x_35.permute(0, 2, 1, 3);  x_35 = None
    attn_output_33 = torch._C._nn.scaled_dot_product_attention(query_layer_11, key_layer_11, value_layer_11, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_11 = key_layer_11 = value_layer_11 = extended_attention_mask = None
    attn_output_34 = attn_output_33.transpose(1, 2);  attn_output_33 = None
    attn_output_35 = attn_output_34.reshape(1, 512, 768);  attn_output_34 = None
    hidden_states_88 = self.L__self___encoder_layer_11_attention_output_dense(attn_output_35);  attn_output_35 = None
    hidden_states_89 = self.L__self___encoder_layer_11_attention_output_dropout(hidden_states_88);  hidden_states_88 = None
    add_24 = hidden_states_89 + hidden_states_87;  hidden_states_89 = hidden_states_87 = None
    hidden_states_90 = self.L__self___encoder_layer_11_attention_output_LayerNorm(add_24);  add_24 = None
    hidden_states_91 = self.L__self___encoder_layer_11_intermediate_dense(hidden_states_90)
    hidden_states_92 = torch._C._nn.gelu(hidden_states_91);  hidden_states_91 = None
    hidden_states_93 = self.L__self___encoder_layer_11_output_dense(hidden_states_92);  hidden_states_92 = None
    hidden_states_94 = self.L__self___encoder_layer_11_output_dropout(hidden_states_93);  hidden_states_93 = None
    add_25 = hidden_states_94 + hidden_states_90;  hidden_states_94 = hidden_states_90 = None
    hidden_states_95 = self.L__self___encoder_layer_11_output_LayerNorm(add_25);  add_25 = None
    first_token_tensor = hidden_states_95[(slice(None, None, None), 0)]
    pooled_output = self.L__self___pooler_dense(first_token_tensor);  first_token_tensor = None
    pooled_output_1 = self.L__self___pooler_activation(pooled_output);  pooled_output = None
    return pytree.tree_unflatten([hidden_states_95, pooled_output_1], self._out_spec)
    